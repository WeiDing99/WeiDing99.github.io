<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title>Wei Ding</title>
    <meta name="author" content="Wei Ding">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
      <tbody>
        <tr style="padding:0px">
          <td style="padding:0px">
            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:40%;max-width:40%; vertical-align:top; padding-top:100px;">
                    <!-- Photo on the left -->
                    <a href="images/weiding.JPG">
                      <img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/weiding.JPG" class="hoverZoomLink">
                    </a>
                    <!-- Links under the image -->
                    <p style="text-align:center; margin-top: 20px;">
                      <a href="mailto:teresading999@gmail.com">Email</a> &nbsp;/&nbsp;
                      <a href="data/dingwei_resume.pdf">CV</a> &nbsp;/&nbsp;
                      <a href="https://scholar.google.com/citations?user=aVQbNDgAAAAJ&hl=zh-CN&authuser=2">Scholar</a> &nbsp;/&nbsp;
                      <a href="https://x.com/WeiDing_Teresa">Twitter</a>
                    </p>
                  </td>
                  <td style="padding:2.5%;width:63%;vertical-align:middle;">
                    <!-- Text on the right -->
                    <p class="name" style="text-align: center;">Wei Ding 丁为</p>
                    <p>I'm a PhD student at the <a href="https://brain.tsinghua.edu.cn/en/Research1/Research_Centers/AI_of_Brain___Cognition_Center.htm">Artificial Intelligence of Brain and Cognition Center</a> at Tsinghua University. Currently, I am a visiting <a href="https://www.tsaylab.com/team">Physical Intelligence Lab</a> at Carnegie Mellon University.</p>
            
                    <p>My research interests lie at the intersection of cognitive neuroscience, learning, and robotics. I am particularly focused on how robots can acquire, adapt, and retain motor skills similarly to how humans do. Inspired by the Language of Thought theory, my work is trying to explore the primitives of motor skills, both at the perception level (such as dominant features of object affordance) and the movement level (such as muscle synergies). By leveraging VLMs, I aim to build generalized robotic skills that can be adapted and transferred across a variety of tasks based on these primitives. Additionally, I'm working on the RLHF process of <a href="https://github.com/THUDM/CogVLM2">CogVLM</a>.</p>
            
                    <p>I am fortunate to be advised by Prof. <a href="https://scholar.google.com/citations?user=xPoVpSEAAAAJ&hl=zh-CN">Jia Liu</a> at <a href="https://collegeai.tsinghua.edu.cn">College of AI</a> and <a href="https://brain.tsinghua.edu.cn">Tsinghua Laboratory of Brain and Intelligence</a>. Previously, I received my Bachelor's Degree from the <a href="https://www.tsinghua.edu.cn/dpien/">Department of Precision Instrument</a> at Tsinghua University, where I was advised by Prof. <a href="https://scholar.google.com/citations?user=qCfE--MAAAAJ&hl=zh-CN">Guoqi Li</a> and Prof. <a href="https://mcgovern.life.tsinghua.edu.cn/en/infoshow-2177.html">Luping Shi</a>.</p>
                  </td>
                </tr>
              </tbody>
            </table>
            

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td style="padding:20px;width:100%;vertical-align:middle">
                    <h2>Research</h2>
                  </td>
                </tr>
              </tbody>
            </table>

            <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr style="padding:0px">
                  <td style="padding:2.5%;width:25%;vertical-align:top;">
                    <div class="one">
                      <div class="two" id='atom_image'>
                        <!-- Video that plays automatically -->
                        <video width="100%" autoplay muted loop>
                          <source src="images/atomdown.mp4" type="video/mp4">
                          Your browser does not support the video tag.
                        </video>
                      </div>
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:top;">
                    <a href="https://arxiv.org/abs/2406.08455">
                      <span class="papertitle">AToM-Bot: Embodied Fulfillment of Unspoken Human Needs with Affective Theory of Mind</span>
                    </a>
                    <br>
                    <strong>Wei Ding</strong>, Fanhong Li, Ziteng Ji, Zhengrong Xue, Jia Liu*
                    <br>
                    <em>RSS 2024, Social Intelligence in Humans and Robots Workshop Spotlight</em>
                    <p>A proactive robot-human interaction framework that uses Vision Language Models and Affective Theory of Mind to autonomously detect and fulfill unspoken human needs, achieving high human satisfaction in need detection and task execution.</p>
                  </td>
                </tr>
            
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:top;">
                    <div class="one">
                      <div class="two" id='hippo_image'><img src='images/hippo.png' width=100%></div>
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:top;">
                    <a href="https://www.biorxiv.org/content/10.1101/2024.01.29.577883v2.full.pdf">
                      <span class="papertitle">Scale-Dependent Coding of the Hippocampus in Relational Memory</span>
                    </a>
                    <br>
                    <strong>Wei Ding</strong>, Yijing Lin, Bo Zhang*, Jia Liu*
                    <br>
                    <em>IJCAI 2024, Human Brain and Artificial Intelligence Workshop</em>
                    <p>This study reveals a functional gradient along the hippocampus that encodes detailed experiences into abstract knowledge for decision-making, showing that the posterior hippocampus handles memory formation while the anterior hippocampus utilizes abstract knowledge for future planning.</p>
                  </td>
                </tr>
            
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:top;">
                    <div class="one">
                      <div class="two" id='reinforce_image'><img src='images/reinforce.png' width=100%></div>
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:top;">
                    <a href="">
                      <span class="papertitle">Revealing the Efficient Coding Strategy of Humans in Relational Memory by Reinforcement Learning</span>
                    </a>
                    <br>
                    Yijing Lin#, <strong>Wei Ding#</strong>, Jia Liu*
                    <br>
                    <em>In Submission</em>
                    <p>This study uses computational modeling to show how selective attention and contextual inference help construct efficient relational memory, enabling humans to navigate complex information while overcoming memory capacity limitations.</p>
                  </td>
                </tr>
            
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:top;">
                    <div class="one">
                      <div class="two" id='imagenet_image'><img src='images/esimagenet.gif' width=100%></div>
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:top;">
                    <a href="https://arxiv.org/pdf/2110.12211">
                      <span class="papertitle">ES-imagenet: A Million Event-Stream Classification Dataset for Spiking Neural Networks</span>
                    </a>
                    <br>
                    Yihan Lin, <strong>Wei Ding</strong>, Shaohua Qiang, Lei Deng, Guoqi Li
                    <br>
                    <em>Frontiers in Neuroscience, 2021</em>
                    <p>This work introduces ES-ImageNet, a large-scale event-stream dataset generated from ILSVRC2012 using the Omnidirectional Discrete Gradient (ODG) algorithm, providing a low-cost, high-speed solution for advancing neuromorphic vision processing with spiking neural networks.</p>
                  </td>
                </tr>
            
                <tr>
                  <td style="padding:20px;width:25%;vertical-align:top;">
                    <div class="one">
                      <div class="two" id='snn_image'><img src='images/snn.png' width=100%></div>
                    </div>
                  </td>
                  <td style="padding:20px;width:75%;vertical-align:top;">
                    <a href="https://www.sciencedirect.com/science/article/pii/S0893608020302902">
                      <span class="papertitle">Comparing SNNs and RNNs on Neuromorphic Vision Datasets: Similarities and Differences</span>
                    </a>
                    <br>
                    Weihua He et al.
                    <br>
                    <em>Neural Networks, 2020</em>
                    <p>This study systematically compares spiking neural networks (SNNs) and recurrent neural networks (RNNs) on neuromorphic vision data, highlighting their similarities and differences in spatiotemporal processing through unified benchmarking experiments.</p>
                  </td>
                </tr>
              </tbody>
            </table>
            
            <table style="width:95%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
              <tbody>
                <tr>
                  <td>
                    <!-- Add margin-top for space above and margin-bottom for space below -->
                    <h2 style="margin-top:40px; margin-bottom:0px;">Honor</h2>
                  </td>
                </tr>
              </tbody>
            </table>
            
            <table width="100%" align="center" border="0" cellpadding="0">
              <tbody>
                <tr>
                  <td colspan="2" style="padding-left:20px; vertical-align:middle;">
                    <ul style="list-style-type: none; padding-left: 0;">
                      <li style="margin-bottom: 10px;">
                        <strong>Outstanding Graduate</strong> (Top 1%), <em>Beijing 2021</em>
                      </li>
                      <li style="margin-bottom: 10px;">
                        <strong>Honored Graduate</strong> (Top 5%), <em>Tsinghua University 2021</em>
                      </li>
                      <li style="margin-bottom: 10px;">
                        <strong>All-Rounder Scholarships</strong> (Top 5%), <em>Department of Precision Instruments 2018, 2019, 2020</em>
                      </li>
                      <li style="margin-bottom: 10px;">
                        <strong>Annual Top Community Service Leader</strong>, <em>Department of Precision Instruments 2018</em>
                      </li>
                      <li style="margin-bottom: 10px;">
                        <strong>Academic Excellence Scholarship</strong>, <em>Department of Precision Instruments 2017</em>
                      </li>
                      <li style="margin-bottom: 10px;">
                        <strong>Silver Medal</strong> in the <em>National High School Students Biology Competition</em>, <em>China 2016</em>
                      </li>
                    </ul>
                  </td>
                </tr>
              </tbody>
            </table>
            

    </table>
    <!-- Footer Section -->
<div style="text-align:center; margin-top:20px;">
  <p>Template from <a href="https://jonbarron.info/" target="_blank">Jon Barron</a></p>
</div>
  </body>
</html>
